{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. SequenceExample - *make_example*, *parse_example* functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_example(sequence, labels):\n",
    "    \"\"\"\n",
    "    create a single tf.train.SequenceExample obj instance given a single input sample\n",
    "    in the form of sequence & labels pair.\n",
    "    \n",
    "    tf.train.SequenceExample consists of SequenceExample.context & SequenceExample.feature_lists\n",
    "    to distinguish between non-sequential and sequential features\n",
    "    \"\"\"\n",
    "    # init the tf.example object\n",
    "    ex = tf.train.SequenceExample()\n",
    "    \n",
    "    # # preprocess sequence\n",
    "    # sequence = preprocess_seq(sequence)\n",
    "    # convert sequence (string) to list of tokens ex.[string, ..., string]\n",
    "    sequence = sequence.split()\n",
    "    sequence_length = len(sequence)\n",
    "    \n",
    "    # add to context (non-sequential) features\n",
    "    ex.context.feature[\"seq_length\"].int64_list.value.append(sequence_length)\n",
    "    ex.context.feature[\"labels\"].bytes_list.value.append(np.array(labels).tostring())\n",
    "        \n",
    "    # add to sequential features\n",
    "    fl_tokens = ex.feature_lists.feature_list[\"tokens\"]\n",
    "    # Populate the tokens in sequence one-by-one\n",
    "    for token in sequence:\n",
    "        fl_tokens.feature.add().bytes_list.value.append(token.encode('utf-8'))\n",
    "    return ex\n",
    "\n",
    "\n",
    "def parse_exmp(serial_exmp):\n",
    "    \"\"\"\n",
    "    instructions to parse a single serialized example object, returns the \n",
    "    seq_length, labels, and tokens values\n",
    "    \"\"\"\n",
    "    context_features = {\n",
    "        'seq_length': tf.FixedLenFeature([], tf.int64),\n",
    "        'labels': tf.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    sequence_features = {\n",
    "        # KEYPOINT: using tf.VarLenFeature here because our string sequence consist of\n",
    "        #           varying number of tokens\n",
    "        'tokens': tf.VarLenFeature(tf.string)\n",
    "        \n",
    "        # # NOTE: Alternative, one could use tf.FixedLenSequenceFeature if some \n",
    "        # # preprocessing step is done to pre-pad the sequence to ensure fixed sequenc \n",
    "        # # length. Downside of this is excessive memory used to store the paddings.\n",
    "        # 'tokens': tf.FixedLenSequenceFeature([], tf.string)\n",
    "    }\n",
    "    \n",
    "    context_ft_parsed, sequence_ft_parsed = tf.parse_single_sequence_example(\n",
    "        serialized=serial_exmp,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "    \n",
    "    seq_length = context_ft_parsed['seq_length']\n",
    "    label = tf.decode_raw(context_ft_parsed['labels'], tf.int64)\n",
    "\n",
    "    # the output of VarLenFeature is a sparse_tensor, so we need to convert it to dense tensor\n",
    "    # note that default value of default_value is integer 0, since our tokens are of tf.string\n",
    "    # dtype, we need to specify custom string value to fill in the missing entries in the sparse\n",
    "    # tensor\n",
    "    tokens = tf.sparse_tensor_to_dense(sp_input=sequence_ft_parsed['tokens'],\n",
    "                                       default_value='')\n",
    "    tokens = tf.reshape(tokens, shape=(-1,))\n",
    "    tokens = tf.transpose(tokens)\n",
    "    \n",
    "    return tokens, label, seq_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example usage of make_example and parse_exmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized Example:\n",
      " b'\\n\\xc9\\x06\\n\\x13\\n\\nseq_length\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\xb1\\x06\\n\\x06labels\\x12\\xa6\\x06\\n\\xa3\\x06\\n\\xa0\\x06\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x12+\\n)\\n\\x06tokens\\x12\\x1f\\n\\t\\n\\x07\\n\\x05test,\\n\\t\\n\\x07\\n\\x05test,\\n\\x07\\n\\x05\\n\\x03113'\n",
      "\n",
      "Tensors Returned From Parse_exmp:\n",
      "tokens:  Tensor(\"transpose:0\", shape=(?,), dtype=string)\n",
      "label:  Tensor(\"DecodeRaw:0\", shape=(?,), dtype=int64)\n",
      "seq_length:  Tensor(\"ParseSingleSequenceExample/ParseSingleSequenceExample:1\", shape=(), dtype=int64)\n",
      "\n",
      "Evaluated Tensor Objects:\n",
      "tokens:  [b'test,' b'test,' b'113']\n",
      "label:  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "sequence length:  3\n"
     ]
    }
   ],
   "source": [
    "# unit tests\n",
    "\n",
    "# make a single tf example obj\n",
    "# note: labels\n",
    "ex = make_example(sequence='test, test, 113',\n",
    "                  labels=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# serialize the tf example obj\n",
    "ex_serial = ex.SerializeToString()\n",
    "print('Serialized Example:\\n', ex_serial)\n",
    "print()\n",
    "\n",
    "# parse the serialized example obj\n",
    "print('Tensors Returned From Parse_exmp:')\n",
    "tokens, label, seq_length = parse_exmp(ex_serial)\n",
    "\n",
    "# examine obj type\n",
    "print('tokens: ',tokens)\n",
    "print('label: ',label)\n",
    "print('seq_length: ',seq_length)\n",
    "print()\n",
    "\n",
    "\n",
    "# evaluate each obj\n",
    "print('Evaluated Tensor Objects:')\n",
    "with tf.Session() as sess:\n",
    "    print('tokens: ', tokens.eval())\n",
    "    print('label: ', label.eval())\n",
    "    print('sequence length: ', seq_length.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Create a TFRecord file from input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tfrecord(data, outf_nm='my_dataset'):\n",
    "    \"\"\"\n",
    "    data is in the format of tuple (sequences, labels), where each sequences and labels\n",
    "    are list objects of string sequence and label, respectively\n",
    "    \"\"\"\n",
    "    feats, labels = data\n",
    "    outf_nm += '.tfrecord'\n",
    "    tfrecord_wrt = tf.python_io.TFRecordWriter(outf_nm)\n",
    "    n_samples = len(labels)\n",
    "    for i in range(n_samples):\n",
    "        exmp = make_example(feats[i], labels[i])\n",
    "        exmp_serial = exmp.SerializeToString()\n",
    "        tfrecord_wrt.write(exmp_serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumed given dataset format\n",
    "\n",
    "train_sequences = ['the unbreakable “slim neck” replacement head contains wide, fanned bristles to help invigorate gum tissue for reduced chances of bleeding/receding gums and enamel erosion and is best to reach tight spaces. slim, tapered, 100% vegetable based nylon bristles reduce dependency on fossil fuels/petroleum and offer a deep clean and massage of the gumline. replacement heads are bpa-free and available in supersoft, soft, and medium bristle and can be used with the radius source toothbrush and tour travel toothbrush. all radius toothbrushes are manufactured in the usa on low-energy machines, are cruelty free, leaping bunny certified, and are 100% satisfaction guaranteed.',\n",
    "                   'the source toothbrush utilizes replacement head technology which reduces toothbrush waste by 93%! the unbreakable “slim neck” replacement head contains wide, fanned bristles to help invigorate gum tissue for reduced chances of bleeding/receding gums and enamel erosion and is best to reach tight spaces. the upcycled #5 one-of-a-kind handle contains wood, paper, or money for a beautiful, natural look & feel. reversible right or left handed design for ergonomic brushing helps reduce pressure on teeth and gums while the 100% vegetable based nylon bristles reduce dependency on fossil fuels/petroleum. the source toothbrush & replacement heads are bpa-free and available in supersoft, soft, and medium bristle. all radius toothbrushes are manufactured in the usa on low-energy machines, are cruelty free, leaping bunny certified, and are 100% satisfaction guaranteed.',\n",
    "                   'this mild foaming cleanser gently removes oil and other residues while oil-free moisturizers keep skin soft, never dry. aloe vera and extracts of cucumber, sea kelp, birch bark and lavender refresh the skin. ph 5.0 / vegan / gluten free.'\n",
    "                  ]\n",
    "\n",
    "train_labels = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "               ]\n",
    "\n",
    "dataset = (train_sequences, train_labels)\n",
    "\n",
    "# create TFRecord file\n",
    "make_tfrecord(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Using the TFRecord file to create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a tf dataset obj from the TFRecord file\n",
    "dataset = tf.data.TFRecordDataset('my_dataset.tfrecord')\n",
    "\n",
    "# use dataset.map() in conjunction with the parse_exmp function created earlier\n",
    "# to de-serialize each example record in TFRecord file\n",
    "dataset = dataset.map(parse_exmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the dataset to set # of epoch, shuffle, and batch size \n",
    "epochs = 3\n",
    "buffer_size = len(train_labels)\n",
    "batch_size = 2\n",
    "\n",
    "# KEYPOINT: \n",
    "# 1 - our dataset consists of varying sequence length and we opt for dynamic padding using dataset.padded_batch \n",
    "# 2 - we specify the manner of padding by using variable \"padded_shapes\", which takes the format:\n",
    "#     [] if the member to parse is of type scalar\n",
    "#     [max_length] if the member to parse is of type list\n",
    "#     [d1,...,dn] if the member is of type array (where d1 specifies first dimension)\n",
    "#     NOTE: the order of the type of padding need to match with the order of the returned data members from\n",
    "#           parse_exmp\n",
    "padded_shapes = ([500],[100],[])\n",
    "\n",
    "# configure dataset epoch, shuffle, padding and batching operations\n",
    "dataset = dataset.repeat(epochs).shuffle(buffer_size).padded_batch(batch_size, padded_shapes=padded_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.string, tf.int64, tf.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(500)]),\n",
       " TensorShape([Dimension(None), Dimension(100)]),\n",
       " TensorShape([Dimension(None)]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Tensor Objects:\n",
      "tokens:  [[b'this' b'mild' b'foaming' b'cleanser' b'gently' b'removes' b'oil' b'and'\n",
      "  b'other' b'residues' b'while' b'oil-free' b'moisturizers' b'keep' b'skin'\n",
      "  b'soft,' b'never' b'dry.' b'aloe' b'vera' b'and' b'extracts' b'of'\n",
      "  b'cucumber,' b'sea' b'kelp,' b'birch' b'bark' b'and' b'lavender'\n",
      "  b'refresh' b'the' b'skin.' b'ph' b'5.0' b'/' b'vegan' b'/' b'gluten'\n",
      "  b'free.' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'']\n",
      " [b'the' b'unbreakable' b'\\xe2\\x80\\x9cslim' b'neck\\xe2\\x80\\x9d'\n",
      "  b'replacement' b'head' b'contains' b'wide,' b'fanned' b'bristles' b'to'\n",
      "  b'help' b'invigorate' b'gum' b'tissue' b'for' b'reduced' b'chances' b'of'\n",
      "  b'bleeding/receding' b'gums' b'and' b'enamel' b'erosion' b'and' b'is'\n",
      "  b'best' b'to' b'reach' b'tight' b'spaces.' b'slim,' b'tapered,' b'100%'\n",
      "  b'vegetable' b'based' b'nylon' b'bristles' b'reduce' b'dependency' b'on'\n",
      "  b'fossil' b'fuels/petroleum' b'and' b'offer' b'a' b'deep' b'clean' b'and'\n",
      "  b'massage' b'of' b'the' b'gumline.' b'replacement' b'heads' b'are'\n",
      "  b'bpa-free' b'and' b'available' b'in' b'supersoft,' b'soft,' b'and'\n",
      "  b'medium' b'bristle' b'and' b'can' b'be' b'used' b'with' b'the' b'radius'\n",
      "  b'source' b'toothbrush' b'and' b'tour' b'travel' b'toothbrush.' b'all'\n",
      "  b'radius' b'toothbrushes' b'are' b'manufactured' b'in' b'the' b'usa'\n",
      "  b'on' b'low-energy' b'machines,' b'are' b'cruelty' b'free,' b'leaping'\n",
      "  b'bunny' b'certified,' b'and' b'are' b'100%' b'satisfaction'\n",
      "  b'guaranteed.' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'']]\n",
      "label:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "sequence length:  [ 40 100]\n"
     ]
    }
   ],
   "source": [
    "# create a one-shot iterator to parse out one single record example at a time\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "tokens, label, seq_length = iterator.get_next()\n",
    "\n",
    "# evaluate each obj\n",
    "print('Evaluated Tensor Objects:')\n",
    "with tf.Session() as sess:\n",
    "    print('tokens: ', tokens.eval())\n",
    "    print('label: ', label.eval())\n",
    "    print('sequence length: ', seq_length.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
