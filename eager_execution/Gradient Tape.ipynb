{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet references: https://www.tensorflow.org/tutorials/eager/automatic_differentiation\n",
    "\n",
    "- Every differentiable operation in Tensorflow has an associated gradient function (ex. the gradient function of `tf.square(x)` would be a function that returns `2.0 * x`). \n",
    "\n",
    "\n",
    "- The way gradient computation for any user-defined function (composed of tensorflow primitive operations) is done is by first \"recording\" all the operations applied to comput the function output. \n",
    "\n",
    "\n",
    "- It then uses that tape and the gradients functions associated with each primitive operation to compute the gradients of the user-defined function using reverse mode differentiation.\n",
    "\n",
    "\n",
    "- When it is inconvenient to wrap all computation in a function, all intermediate calculation operations can be recorded within an explicit `tf.GradientTape` context wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient Tape Context Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dy: 8.0 \n",
      "\n",
      "dz/dx:\n",
      " [[8. 8.]\n",
      " [8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "# gradient tape context\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x) # watch all operations done on x\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the gradient tape to compute the derivative of z with respect to y\n",
    "dz_dy = t.gradient(z, y)\n",
    "# Use the gradient tape to compute the derivative of z with respect to x\n",
    "dz_dx = t.gradient(z, x)\n",
    "\n",
    "print('dz/dy:',dz_dy.numpy(),'\\n')\n",
    "print('dz/dx:\\n',dz_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order gradients can be computed by wrapper one gradient tape context within another as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: 3.0\n",
      "d2y/dx2: 6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)  # Convert the Python 1.0 to a Tensor object\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        t2.watch(x)\n",
    "        y = x * x * x\n",
    "    # Compute the gradient inside the 't' context manager, which means \n",
    "    # this gradient computation is differentiable under t\n",
    "    dy_dx = t2.gradient(y, x)\n",
    "    \n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "\n",
    "print('dy/dx:', dy_dx.numpy())\n",
    "print('d2y/dx2:', d2y_dx2.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
